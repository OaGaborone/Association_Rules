**********************************
a) Your name and your partner's name and Columbia UNI;
**********************************


**********************************
b) A list of all the files that you are submitting;
**********************************
1. INTEGRATED-DATASET.csv - The dataset on which the apriopri algorithm will be run to generate association rules
2. 311-Dataset.csv - The parent dataset of INTEGRATED-DATASET.csv. This dataset is a small subset of the "311 Service Requests from 2010 to Present" from the NYC OpenData database.
3. datasetparser.py - The python script used to create INTEGRATED-DATASET.csv from 311-Dataset.csv 
4. apriori.py - the core algorithm
5. rules.py - the "main" program
6. README - This file
7. Makefile - A dummy makefile with a 'howto', since no compilation is required

**********************************
c) A detailed description explaining: (a) which NYC Open Data data set(s) you used to generate the INTEGRATED-DATASET file; (b) what (high-level) procedure you used to map the original NYC Open Data data set(s) into your INTEGRATED-DATASET file; (c) what makes your choice of INTEGRATED-DATASET file interesting (in other words, justify your choice of NYC Open Data data set(s)). The explanation should be detailed enough to allow us to recreate your INTEGRATED-DATASET file exactly from scratch from the NYC Open Data site;
**********************************

(a) We used the "311 Service Requests from 2010 to Present" dataset from NYC OpenData to form the base of our INTEGRATED-DATASET. This dataset is about 1.3 GB in size.
(b) The original dataset was far too large for efficient processing and contained orders of magnitude more information than we needed. So we trimmed it down and extracted the data we need from the dataset into INTEGRATED-DATASET.csv. It can be reproduced by doing the following:
	1) Download the entire '311 Service Requests from 2010 to Present ' dataset (1.3 GB)
	2) Truncate the file on the Terminal using the command: 'dd if=dataset.csv of=311-Dataset.csv bs=10m count=1'. Here dataset.csv is the original 1.3 GB dataset and 311-Dataset.csv is the truncated 10 MB dataset. It should contain a little over 20,000 entries.
	3) Run a python script which will strip out all the data except ones pertaining to the fields 'Complaint Type', 'Descriptor' and 'Location Type'. The script does this by using a CSV module, which reads entries line by line and indexes to the relevant data using the ',' as a delimiter. Entries without a zipcode are removed. This can then be written to INTEGRATED-DATASET.csv by running the script and printing the output to it like so: python parser.py > INTEGRATED-DATASET.csv. The script is not included, but can be provided on demand.
	4) Remove the first line in INTEGRATED-DATASET.csv as this entry corresponds to the cell names and is not needed.
(c) We chose the "311 Service Requests from 2010 to Present" dataset as it contains an enormous number of entries, which gives us the flexibility to prune unneeded data and still have a lot remaining to play with. It also has a large number of fields so we had flexibility in which and how many items we could include in the itemset. In the end we settled for about ~18,000 entries and the fields 'Complaint Type', 'Descriptor' and 'Location Type' as we thought we could get some interesting relations from it. For example, we could potentially mine for rules that will tell us what kind of complaints are most prevelant in a location type and what are the most common occurences for a particular complaint type. These results could potentially have real world use, in say determining whether a particular neighborhood is conducive to living.


**********************************
d) A clear description of how to run your program (note that your project must compile/run under Linux in your CS account);
**********************************
1. Go to the directory which contains the code.
2. In command line, type: 
	$ python rules.py -s min_support -c min_conf > output.txt
	Or you can run using the default values:
	$ python rules.py > output.txt
	For example: 
	$ python rules.py -s 0.02 -c 0.2 > output.txt
3. Be sure to redirect output to output.txt, and put all files in the same directory.

**********************************
e) A clear description of the internal design of your project;
**********************************

Large item set computing:
In each round, we generate the k+1 dimension supersets of the k dimension sets in the previous round. After we have obtained a possible k+1 set, we check if any of its subsets are not in the k sets, if so, we delete that k+1 set, and the rest will be candidates. Then from those candidates, we compute their support values and eliminate those with support<min_sup. The rest will be stored and used for later computation.

Rules generating:
For each large item set, we generate all its possible combination of subsets, since the rule candidates size are much smaller than that of the large item sets, to avoid additional space complexity, we do not apply anti-monotone rule here and simply compute each confidence and check if it is larger than min_conf. The computation will be low-cost since all supports can be retrieved in O(1) time.

**********************************
f) The command line specification of an interesting sample run (i.e., a min_sup, min_conf combination that produces interesting results). Briefly explain why the results are interesting.
**********************************

$ python rules.py -s 0.03 -c 0.5 > output.txt

Since we don't have too many recurrence in our data set, do not set the support too high, but we have over 18,000 rows of data to ensure the rules are sensible.

For example, you can find out the in the previous run:

	[No Access] => [Blocked Driveway, Street/Sidewalk] (Conf: 100.0%, Supp: 3.26515192401%)
	[Blocked Driveway, Street/Sidewalk] => [No Access] (Conf: 73.0676328502%, Supp: 4.46867073237%)

This shows almost all no access reports are caused or accompanied by blocked driveway and pertaining to street/sidewalk.This could be potentially helpful in the sense that when we have a next call reporting damaged access, we can send out personnel to fix nearby driveways right away with low delay and high confidence, and after the driveway problem is solved, the result suggests other potential problems, hence to ensure public safety and good transportation service.


**********************************
g) Any additional information that you consider significant.
**********************************

Don't set the min_sup too high, otherwise it won't provide insights.
